name: Daily Database Backup (API Method)

on:
  schedule:
    # Run daily at 2 AM UTC (adjust timezone as needed)
    - cron: '0 2 * * *'
  workflow_dispatch: # Allow manual trigger

jobs:
  backup:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
      
      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      
      - name: Create backup directory
        run: |
          mkdir -p backups
          echo "BACKUP_DATE=$(date +'%Y-%m-%d_%H-%M-%S')" >> $GITHUB_ENV
      
      - name: Install dependencies
        run: |
          npm install @supabase/supabase-js
      
      - name: Create backup script
        run: |
          cat > backup-script.js << 'EOF'
          const { createClient } = require('@supabase/supabase-js');
          const fs = require('fs');

          async function backupDatabase() {
            const supabase = createClient(
              process.env.SUPABASE_URL,
              process.env.SUPABASE_SERVICE_KEY
            );

            console.log('Fetching all tables data...');
            
            const tables = [
              'prayers',
              'prayer_updates',
              'prayer_prompts',
              'prayer_types',
              'email_subscribers',
              'user_preferences',
              'status_change_requests',
              'update_deletion_requests',
              'admin_settings',
              'analytics'
            ];

            const backup = {
              timestamp: new Date().toISOString(),
              version: '1.0',
              tables: {}
            };

            for (const table of tables) {
              try {
                console.log(`Backing up table: ${table}...`);
                const { data, error } = await supabase
                  .from(table)
                  .select('*');
                
                if (error && error.code !== 'PGRST116') {
                  console.error(`Error backing up ${table}:`, error);
                  backup.tables[table] = { error: error.message, data: [] };
                } else {
                  backup.tables[table] = { count: data?.length || 0, data: data || [] };
                  console.log(`✓ Backed up ${data?.length || 0} rows from ${table}`);
                }
              } catch (err) {
                console.error(`Exception backing up ${table}:`, err);
                backup.tables[table] = { error: err.message, data: [] };
              }
            }

            // Save backup as JSON
            const filename = `backups/backup_${process.env.BACKUP_DATE}.json`;
            fs.writeFileSync(filename, JSON.stringify(backup, null, 2));
            console.log(`\n✓ Backup saved to ${filename}`);

            // Create summary
            const summary = {
              timestamp: backup.timestamp,
              date: process.env.BACKUP_DATE,
              type: 'api',
              format: 'json',
              tables: Object.keys(backup.tables).reduce((acc, table) => {
                acc[table] = backup.tables[table].count || 0;
                return acc;
              }, {})
            };

            fs.writeFileSync(
              `backups/backup_${process.env.BACKUP_DATE}_summary.json`,
              JSON.stringify(summary, null, 2)
            );

            console.log('\nBackup Summary:');
            console.log(JSON.stringify(summary, null, 2));
          }

          backupDatabase().catch(err => {
            console.error('Backup failed:', err);
            process.exit(1);
          });
          EOF
      
      - name: Run backup
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_SERVICE_KEY: ${{ secrets.SUPABASE_SERVICE_KEY }}
          BACKUP_DATE: ${{ env.BACKUP_DATE }}
        run: |
          node backup-script.js
      
      - name: Compress backup
        run: |
          cd backups
          gzip backup_${BACKUP_DATE}.json
          echo "✓ Backup compressed"
      
      - name: Commit and push backup
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add backups/
          git commit -m "chore: automated database backup ${BACKUP_DATE}" || echo "No changes to commit"
          git push || echo "No changes to push"
      
      - name: Keep only last 30 backups
        run: |
          cd backups
          # List all backup files by date, skip the 30 most recent, delete the rest
          ls -t backup_*.json.gz 2>/dev/null | tail -n +31 | xargs -r rm
          ls -t backup_*_summary.json 2>/dev/null | tail -n +31 | xargs -r rm
          
          # Commit deletion if any files were removed
          if [ -n "$(git status --porcelain)" ]; then
            git config user.name "GitHub Actions Bot"
            git config user.email "actions@github.com"
            git add .
            git commit -m "chore: cleanup old backups (keep last 30)" || echo "No old backups to remove"
            git push || echo "No changes to push"
          fi
      
      - name: Upload backup as artifact
        uses: actions/upload-artifact@v4
        with:
          name: database-backup-${{ env.BACKUP_DATE }}
          path: backups/backup_${{ env.BACKUP_DATE }}.json.gz
          retention-days: 30
